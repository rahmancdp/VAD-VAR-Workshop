# WatsonX.ai Part 1: Basic navigation and zero shot prompting

Watsonx.ai is a core component of watsonx, IBM's enterprise-ready AI and data platform designed to multiply the impact of AI across a business. The watsonx.ai component makes it possible for enterprises to train, validate, tune, and deploy traditional AI and generative AI models.
 
## Watsonx.ai console

We'll start with a quick explainer of the watsonx.ai console. First, [follow these instructions](/watsonx/watsonxai/100#accessing-watsonxai-from-ibm-cloud) to access the watsonx.ai homepage.

The homepage will look similar to the following:

![](https://github.com/rahmancdp/watsonx-images/blob/1487ad4980be546f363cb86bd78c69fd8c247a55/1.jpg)

These are the various regions of the console.

1. Navigate to the Prompt Lab console. You can experiment with different models, test your prompts, adjust model parameters and save your prompt sessions. This is the focus of this lab.

2. Create AutoAI jobs to automatically build machine learning (ML) models.

3. Create new, or work with existing Python or R notebooks directly in the watsonx.ai UI.

4. Load data and then prepare it (using Data Refinery) for AI consumption.

5. Quick links to recently visited pages

6. Shows a list of projects. For the watsonx.ai free tier, you will see a default project called `{username}'s sandbox`

7. Deployment space - this is where you can add assets in one place to create, run, and manage deployments.

8. A collection of samples. A great place to explore if you are new to watsonx.ai.

9. Model highlights - watsonx.ai will highlight various foundation models and use cases.

## Prompt Lab - Basic Navigation

If this is your first time accessing the prompt lab in this account, you'll be prompted to acknowledge a few points related to generative AI models and optionally take a tour.

![welcome-prompt-lab](./images/101/welcome-prompt-lab.png)

Whether you decide to take the tour or not, you should end up on the prompt lab UI, which is where we will begin!

![](https://github.com/rahmancdp/watsonx-images/blob/1487ad4980be546f363cb86bd78c69fd8c247a55/2.png)

This lab will cover a core subset of the Prompt Lab capabilities. For an initial explanation of the UI, lets walk through the numbered sections:

1. The ability to toggle between **Structured** prompt or **Freeform** prompt editors.

   a. **Structured** prompt is the default and provides guidelines for prompt creation.

   b. **Freeform** prompting shows one text area to interact with the foundation model. Likely preferred by more experienced users.

2. Use the dropdown to choose between different foundation models.

3. A first instruction to be sent to the foundation model. Optional, as you may not always need a top level instruction.

4. Sample input that can be combined with Sample output (item 5) to "teach" the model how to appropriately respond to your prompt.

5. Sample output (corresponding to input from item 4).

   > Foundation models can be thought of as probability machines - they generate output by choosing the next most probable token, given all previous tokens.\ <br/> There are many techniques for improving the output of a foundation model. One of those is to "teach" the model by providing sample input and output (referred to as a "shot"). Types of shots include: <br/><br/>**Zero-shot prompting:** no input/output provided <br/>**One-shot prompting:** a single input/output example provided <br/>**Few-shot prompting:** multiple examples provided.

6. The **Try** section is where you enter your prompt/query.

7. This is where generated output will be displayed.

8. Click the **Generate** button when you're ready for the foundation model to receive your inputs.

9. Watsonx.ai provides AI guardrails. By default, it is off. You can turn this on to prevent potential harmful input and output text (such as hate, abuse, or prejudiced wordings).

Other controls, like updating inference configuration parameters, will be discussed as we progress through the labs.

## Exploring foundation models with a zero-shot prompt

These are the available foundation models in watsonx.ai as of March 2024.

| Model                       | Architecture    | Parameters | Trained by               | Usage                                                                                                            |
| --------------------------- | --------------- | ---------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------- |
| Starcoder-15-5b \*          | Decoder only    | 15.5b      | BigCode                  | Code generation, Code conversion                                                                                 |
| mt0-xxl-13b                 | Encoder-decoder | 13b        | BigScience               | Generation, Summarization, Classification, Question Answering                                                    |
| codellama-34b-instruct-hf   | Encoder-decoder | 34b        | Code Llama               | Code generation and conversion                                                                                   |
| flan-t5-xl-3b               | Encoder-decoder | 3b         | Google                   | Generation, Extraction, Summarization, Classification, Question Answering, RAG                                   |
| flan-t5-xxl-11b             | Encoder-decoder | 11b        | Google                   | Generation, Extraction, Summarization, Classifciation, Question Answering, RAG                                   |
| flan-ul2-20b                | Encoder-decoder | 20b        | Google                   | Generation, Extraction, Summarization, Classification, Question Answering, RAG                                   |
| mixtral-8x7b-instruct-v01-q | Decoder only    | 46.7b      | Mistral AI, tuned by IBM | Summarization, RAG, Classification, Generation, Code generation and conversion, Extraction                       |
| llama-2-13b-chat            | Decoder only    | 13b        | Meta                     | Generation, Extraction, Summarization, Classification, Question Answering, RAG, Code generation, Code conversion |
| llama-2-70b-chat            | Decoder only    | 70b        | Meta                     | Generation, Extraction, Summarization, Classification, Question Answering, RAG, Code generation, Code conversion |

\* Deprecated

More will be added as other foundation models are vetted and deemed appropriate for watsonx.ai.

> There are also several IBM models available, the granite-13b-chat-v2, granite-13b-instruct-v2 and granite-20b-multilingual models, which will be covered in detail in a future iteration of this lab.

1. On the <span>left hand panel</span>, click the sample prompt icon, **]⃞\[**.
 

   ![sample_prompts](./images/101/sample-prompts-icon.png)

   **Watsonx.ai** provides sample prompts grouped into categories like:

   - Summarization
   - Classification
   - Generation
   - Extraction
   - Question Answering
   - Code
   - Translation

   These are the 7 main use cases for generative AI. For the following tests, we will utilize the **Marketing email generation** sample in the **Generation** section.

2. Select the **Marketing email generation** from the list of examples on the left. This prompt requests a 5 sentence marketing message based on the provided characteristics.

   ![marketing_email_gen](./images/101/marketing-email-generation.png)

   > Notice how the model **flan-t5-xxl-11b** was automatically selected for this sample use case. Watsonx.ai selects the model that is most likely to provide the best performance. However, this is not a guarantee, and in this part of the lab, we will explore different models on this same prompt.

3. Click the **Details** field in the **Try** section to expand the box and see the full text of this example.

   ![marketing_email_details](./images/101/marketing-email-details.png)

   If you cannot find this prompt example, or if the contents have changed, you can enter:

   - For **Instruction**

   ```
   Generate a 5 sentence marketing message for a company with the given characteristics.
   ```

   - For **Details** under the **Try** section

   ```txt
   Characteristics:
   Company - Golden Bank
   Offer includes - no fees, 2% interest rate, no minimum balance
   Tone - informative
   Response requested - click the link
   End date - July 15
   ```

4. Click **Generate** to see the email output.

   ![generate_output](./images/101/marketing-email-output.png)

   This is a reasonable output – but perhaps not yet ideal.

   > Note: This was a zero-shot prompt, as we did not provide any sample input/output.

5. Look to the left of the **Generate** button and you will see text similar to the following:

   ![generate_stats](./images/101/marketing-email-gen-stats.png)

   Note: All the text used in the Instruction and Details sections becomes part of the prompt. For this model, the maximum tokens allowed for one transaction is 4096. This varies depending on model.


##  summary

- We learned how use a sample prompt with different foundation models.
- Even with zero-shot prompting, the prompt input can be modified to get a better response from foundation models.

# WatsonX.ai  Part 2: One-shot prompting and saving your work

This is Part 3 of the watsonx.ai L3 badge lab. In this lab, we will show how providing an example input and output (one-shot prompting) can lead to better output. Additionally, we will show how to save your work as a single prompt, as an entire prompt session, or as a Jupyter notebook.

## One-shot prompting

Thus far, we have used zero-shot prompting and utilized parameter updates, prompt instructions, and thoughtful model selection to obtain desired outputs. Now, let's see how providing a specific example of the desired output for a given input can further improve model output.

1. If you're not already there, navigate back to the Prompt Lab, and [open a new Prompt Lab session](/watsonx/watsonxai/100#creating-a-new-prompt-lab-session)

2. Select the **flan-ul2-20b** model and open the **Model parameters** slide out panel.

3. Change the number of **Max tokens** from **200** to **100**.

   ![change_maxtokens](./images/103/change-maxtokens.png)

 
4. Copy and paste the following text into the **Set up** section's **Input** field in the optional **Examples** section:

   ```txt
   The following paragraph is a consumer complaint.
   The complaint is about one of these options: credit cards, credit reporting, mortgages and loans, retail banking, or debt collection. Read the following paragraph and list all the issues.

   I bought a GPS from your store and the instructions included are in Spanish, not English. I have to use Google Translate to figure it out. The mounting bracket was broken, and so I need information on how to get a replacement. Moreover, the information seems to be outdated because I cannot see the new roads put in around my house within the last 12 months.
   ```

5. Copy and paste the following text into the **Set up** section's **Output** field in the optional **Examples** section:

   ```txt
   The list of issues is as follows:
   1) The instructions are in Spanish, not English.
   2) The mounting bracket is broken.
   3) The information is outdated.
   ```

   Your screen should look like this:
   ![one_shot_setup](./images/103/one-shot-setup.png)

   Click **Generate**. You should now see the following output:

   ![one_shot_output](./images/103/one-shot-output.png)

   Now we have vastly improved output. The completion listed all valid issues and put them in a numbered list.

    <Warning text='Do NOT close out the session or remove any information. You will need this for the next section.'/>


# WatsonX.ai part-3 with Langchain


[Langchain](https://docs.langchain.com/docs/) is a framework which helps in developing more complex AI powered apps. It works with many language models, and provides a set of tools to make intricate logic more manageable, such as sending system and human instructions, dynamic prompts using templates, chaining, output parsing, and more.

- Langchain python module: https://pypi.org/project/langchain/
- Supports Python and Javascript / Typescript

Today, we will be using Langchain with [watsonx.ai](https://www.ibm.com/products/watsonx-ai), and the [IBM Watson Machine Learning SDK](https://ibm.github.io/watson-machine-learning-sdk/), specifically the SDK's [Langchain extension](https://ibm.github.io/watson-machine-learning-sdk/fm_extensions.html#langchain). Langchain already provides support for over 70 LLMs, but in case you want to support a new one, there is also custom LLM functionality. Read more [here](https://python.langchain.com/docs/modules/model_io/models/llms/custom_llm)

The first part starts with a gentle introduction to some langchain capabilities, including how to initialize a model, change inference parameters, use templates, chains, and load documents. We will then move on to summarization of large amounts of text, which includes more moving parts.

## There are 3 ways you can run these Langchain labs:

1. Locally on your laptop (requires more technical expertise)
2. In our JupyterHub server environment (easiest)
3. In the watsonx UI (Work with data and models in Python or R notebooks)

## 1: Run the labs locally on your laptop:

### Prerequisites

1. Make sure Python >= 3.9 is installed. (`python --version` / `python3 --version`)
   - If you are running Windows, make sure Microsoft C++ Build tools are installed.
2. Download [langchain.zip](https://github.com/ibm-build-lab/VAD-VAR-Workshop/blob/main/content/labs/Watsonx/WatsonxAI/105/langchain.zip), decompress it, and change to the directory, (`cd langchain`).
3. In your terminal, create a Virtual Environment: `python3 -m venv env`.
4. Activate the environment:
   - MacOS + Linux: `source env/bin/activate`
   - Windows:
      <pre>
         `# If using Windows cmd.exe: (command prompt)`
         `env\Scripts\activate.bat`
         `# If using Windows PowerShell:`
         `env\Scripts\Activate.ps1`
      </pre>
   - You can deactivate it later with `deactivate` when done with the labs.
5. Install the requirements: `python3 -m pip install -r requirements.txt`
6. Start your JupyterHub server with `jupyter notebook` and then run the "env-test.ipynb" to make sure there aren't any issues.
7. Using the `.env.example` file, create an `.env` file in your folder.
8. Fill out the values in the `.env` file:


   - `API_KEY` can be found at [cloud.ibm.com/iam/apikeys](https://cloud.ibm.com/iam/apikeys) after logging in. (If you don't have access to create an API key, contact your IBM cloud account Admin.)
   - `IBM_CLOUD_URL` should be your regional IBM cloud URL like in `.env.example`. You can find this value by clicking the "View code" button next to the model dropdown in the Prompt Lab. You should use the URL **without** any paths (aka https://us-south.ml.cloud.ibm.com).

![Instance url](./images/105/instance-url.png)

![Instance url](./images/105/instance-url-2.png)

- `PROJECT_ID` can be found under [watsonx projects](https://dataplatform.cloud.ibm.com/projects/?context=wx) under the project manage tab. The id is also part of the URL: `https://dataplatform.cloud.ibm.com/projects/<project-id>/manage/general?context=wx`

After finishing the prerequisites, complete the labs with the jupyter notebooks below. If you run into any issues, consult the [Troubleshooting](/watsonx/watsonxai/troubleshooting) page.


# Sample Usecases

[Chat with watsonx.ai foundational model](https://huggingface.co/spaces/RAHMAN00700/rahmans_watsonx)


# WatsonX.ai with watsonx assistant
[Connect watsonx.ai with watsonx assistant](https://abdulrahmanh.com/blog/How-to-connect-Watsonx-Assistant-to-WatsonX)

    

 
