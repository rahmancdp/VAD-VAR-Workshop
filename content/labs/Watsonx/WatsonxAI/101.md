# WatsonX.ai Part 1: Basic navigation and zero shot prompting

Watsonx.ai is a core component of watsonx, IBM's enterprise-ready AI and data platform designed to multiply the impact of AI across a business. The watsonx.ai component makes it possible for enterprises to train, validate, tune, and deploy traditional AI and generative AI models.
 
## Watsonx.ai console

We'll start with a quick explainer of the watsonx.ai console. First, [follow these instructions](/watsonx/watsonxai/100#accessing-watsonxai-from-ibm-cloud) to access the watsonx.ai homepage.

The homepage will look similar to the following:

![](https://github.com/rahmancdp/watsonx-images/blob/1487ad4980be546f363cb86bd78c69fd8c247a55/1.jpg)

These are the various regions of the console.

1. Navigate to the Prompt Lab console. You can experiment with different models, test your prompts, adjust model parameters and save your prompt sessions. This is the focus of this lab.

2. Create AutoAI jobs to automatically build machine learning (ML) models.

3. Create new, or work with existing Python or R notebooks directly in the watsonx.ai UI.

4. Load data and then prepare it (using Data Refinery) for AI consumption.

5. Quick links to recently visited pages

6. Shows a list of projects. For the watsonx.ai free tier, you will see a default project called `{username}'s sandbox`

7. Deployment space - this is where you can add assets in one place to create, run, and manage deployments.

8. A collection of samples. A great place to explore if you are new to watsonx.ai.

9. Model highlights - watsonx.ai will highlight various foundation models and use cases.

## Prompt Lab - Basic Navigation

If this is your first time accessing the prompt lab in this account, you'll be prompted to acknowledge a few points related to generative AI models and optionally take a tour.

![welcome-prompt-lab](./images/101/welcome-prompt-lab.png)

Whether you decide to take the tour or not, you should end up on the prompt lab UI, which is where we will begin!

![prompt-lab-sections](./images/101/prompt-lab-sections.png)

This lab will cover a core subset of the Prompt Lab capabilities. For an initial explanation of the UI, lets walk through the numbered sections:

1. The ability to toggle between **Structured** prompt or **Freeform** prompt editors.

   a. **Structured** prompt is the default and provides guidelines for prompt creation.

   b. **Freeform** prompting shows one text area to interact with the foundation model. Likely preferred by more experienced users.

2. Use the dropdown to choose between different foundation models.

3. A first instruction to be sent to the foundation model. Optional, as you may not always need a top level instruction.

4. Sample input that can be combined with Sample output (item 5) to "teach" the model how to appropriately respond to your prompt.

5. Sample output (corresponding to input from item 4).

   > Foundation models can be thought of as probability machines - they generate output by choosing the next most probable token, given all previous tokens.\ <br/> There are many techniques for improving the output of a foundation model. One of those is to "teach" the model by providing sample input and output (referred to as a "shot"). Types of shots include: <br/><br/>**Zero-shot prompting:** no input/output provided <br/>**One-shot prompting:** a single input/output example provided <br/>**Few-shot prompting:** multiple examples provided.

6. The **Try** section is where you enter your prompt/query.

7. This is where generated output will be displayed.

8. Click the **Generate** button when you're ready for the foundation model to receive your inputs.

9. Watsonx.ai provides AI guardrails. By default, it is off. You can turn this on to prevent potential harmful input and output text (such as hate, abuse, or prejudiced wordings).

Other controls, like updating inference configuration parameters, will be discussed as we progress through the labs.

## Exploring foundation models with a zero-shot prompt

These are the available foundation models in watsonx.ai as of March 2024.

| Model                       | Architecture    | Parameters | Trained by               | Usage                                                                                                            |
| --------------------------- | --------------- | ---------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------- |
| Starcoder-15-5b \*          | Decoder only    | 15.5b      | BigCode                  | Code generation, Code conversion                                                                                 |
| mt0-xxl-13b                 | Encoder-decoder | 13b        | BigScience               | Generation, Summarization, Classification, Question Answering                                                    |
| codellama-34b-instruct-hf   | Encoder-decoder | 34b        | Code Llama               | Code generation and conversion                                                                                   |
| flan-t5-xl-3b               | Encoder-decoder | 3b         | Google                   | Generation, Extraction, Summarization, Classification, Question Answering, RAG                                   |
| flan-t5-xxl-11b             | Encoder-decoder | 11b        | Google                   | Generation, Extraction, Summarization, Classifciation, Question Answering, RAG                                   |
| flan-ul2-20b                | Encoder-decoder | 20b        | Google                   | Generation, Extraction, Summarization, Classification, Question Answering, RAG                                   |
| mixtral-8x7b-instruct-v01-q | Decoder only    | 46.7b      | Mistral AI, tuned by IBM | Summarization, RAG, Classification, Generation, Code generation and conversion, Extraction                       |
| llama-2-13b-chat            | Decoder only    | 13b        | Meta                     | Generation, Extraction, Summarization, Classification, Question Answering, RAG, Code generation, Code conversion |
| llama-2-70b-chat            | Decoder only    | 70b        | Meta                     | Generation, Extraction, Summarization, Classification, Question Answering, RAG, Code generation, Code conversion |

\* Deprecated

More will be added as other foundation models are vetted and deemed appropriate for watsonx.ai.

> There are also several IBM models available, the granite-13b-chat-v2, granite-13b-instruct-v2 and granite-20b-multilingual models, which will be covered in detail in a future iteration of this lab.

1. On the <span>left hand panel</span>, click the sample prompt icon, **]⃞\[**.
 

   ![sample_prompts](./images/101/sample-prompts-icon.png)

   **Watsonx.ai** provides sample prompts grouped into categories like:

   - Summarization
   - Classification
   - Generation
   - Extraction
   - Question Answering
   - Code
   - Translation

   These are the 7 main use cases for generative AI. For the following tests, we will utilize the **Marketing email generation** sample in the **Generation** section.

2. Select the **Marketing email generation** from the list of examples on the left. This prompt requests a 5 sentence marketing message based on the provided characteristics.

   ![marketing_email_gen](./images/101/marketing-email-generation.png)

   > Notice how the model **flan-t5-xxl-11b** was automatically selected for this sample use case. Watsonx.ai selects the model that is most likely to provide the best performance. However, this is not a guarantee, and in this part of the lab, we will explore different models on this same prompt.

3. Click the **Details** field in the **Try** section to expand the box and see the full text of this example.

   ![marketing_email_details](./images/101/marketing-email-details.png)

   If you cannot find this prompt example, or if the contents have changed, you can enter:

   - For **Instruction**

   ```
   Generate a 5 sentence marketing message for a company with the given characteristics.
   ```

   - For **Details** under the **Try** section

   ```txt
   Characteristics:
   Company - Golden Bank
   Offer includes - no fees, 2% interest rate, no minimum balance
   Tone - informative
   Response requested - click the link
   End date - July 15
   ```

4. Click **Generate** to see the email output.

   ![generate_output](./images/101/marketing-email-output.png)

   This is a reasonable output – but perhaps not yet ideal.

   > Note: This was a zero-shot prompt, as we did not provide any sample input/output.

5. Look to the left of the **Generate** button and you will see text similar to the following:

   ![generate_stats](./images/101/marketing-email-gen-stats.png)

   Note: All the text used in the Instruction and Details sections becomes part of the prompt. For this model, the maximum tokens allowed for one transaction is 4096. This varies depending on model.

   > The currency of an LLM is tokens. **Tokens do not map 1 to 1 with words**. For more information and to see tokenization in action, see [this reference page](/watsonx/watsonxai/ref100#illustrative-example)

 

##  summary

- We learned how use a sample prompt with different foundation models.
- Even with zero-shot prompting, the prompt input can be modified to get a better response from foundation models.
 
