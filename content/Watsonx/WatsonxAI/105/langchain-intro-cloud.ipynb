{"cells":[{"cell_type":"markdown","id":"032112c4","metadata":{},"source":["## 1. Programmatically using WatsonX.ai models"]},{"cell_type":"code","execution_count":22,"id":"515172b9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from lomond->ibm-watson-machine-learning>=1.0.320) (1.16.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic>=1.10.0) (4.12.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n"]}],"source":["\n","\n","import sys\n","\n","!{sys.executable} -m pip install \"ibm-watson-machine-learning>=1.0.320\" | tail -n 1\n","!{sys.executable} -m pip install \"pydantic>=1.10.0\" | tail -n 1\n","!{sys.executable} -m pip install langchain | tail -n 1\n","!{sys.executable} -m pip install langchain-community | tail -n 1\n","!{sys.executable} -m pip install -q sentence-transformers\n","\n","\n"]},{"cell_type":"code","execution_count":3,"id":"f4350954","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pypdf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (4.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pypdf"]},{"cell_type":"code","execution_count":4,"id":"2b0ce0b2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: SQLAlchemy==2.0.29 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (2.0.29)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy==2.0.29) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy==2.0.29) (2.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install SQLAlchemy==2.0.29"]},{"cell_type":"code","execution_count":20,"id":"4adcdb35","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done importing dependencies.\n"]}],"source":["# First import the dependencies we need:\n","***REMOVED***\n","from time import sleep\n","try:\n","    from langchain import PromptTemplate\n","    from langchain.chains import LLMChain, SimpleSequentialChain\n","    from langchain.document_loaders import PyPDFLoader\n","    from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n","    from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n","    from langchain.text_splitter import CharacterTextSplitter # Text splitter\n","\n","    ***REMOVED***\n","    ***REMOVED***\n","    from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n","    \n","except ImportError as e:\n","    print(e)\n","\n","print(\"Done importing dependencies.\")\n"]},{"cell_type":"code","execution_count":6,"id":"96339420","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done getting env variables.\n"]}],"source":["# Get our API key and URL from .env\n","\n","project_id = \"\"\n","api_key = \"\"\n","ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n","\n","\n","\n","if api_key is None or ibm_cloud_url is None or project_id is None:\n","    raise Exception(\"One or more environment variables are missing!\")\n","else:\n","    creds = {\n","        \"url\": ibm_cloud_url,\n","        \"apikey\": api_key \n","***REMOVED***\n","print(\"Done getting env variables.\")\n"]},{"cell_type":"code","execution_count":7,"id":"a51cbd27","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done initializing LLM.\n"]}],"source":["# Initialize the WatsonX model\n","params = {\n","    GenParams.DECODING_METHOD: \"sample\",\n","    GenParams.TEMPERATURE: 0.2,\n","    GenParams.TOP_P: 1,\n","    GenParams.TOP_K: 25,\n","    GenParams.REPETITION_PENALTY: 1.0,\n","    GenParams.MIN_NEW_TOKENS: 1,\n","    GenParams.MAX_NEW_TOKENS: 20\n","}\n","\n","llm_model = Model(\n","    model_id=\"google/flan-ul2\",\n","    params=params,\n","    credentials=creds,\n","    project_id=project_id\n",")\n","print(\"Done initializing LLM.\")\n"]},{"cell_type":"code","execution_count":8,"id":"1b25c008","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The capital of France is Paris\n","The capital of Japan is Tokyo\n","The capital of Australia is Canberra\n"]}],"source":["# Predict with the model\n","countries = [\"France\", \"Japan\", \"Australia\"]\n","\n","try:\n","  for country in countries:\n","    question = f\"What is the capital of {country}\"\n","    res = llm_model.generate_text(question)\n","    print(f\"The capital of {country} is {res.capitalize()}\")\n","except Exception as e:\n","  print(e)\n"]},{"cell_type":"markdown","id":"a52e2e8f","metadata":{},"source":["## 2. Prompt Templates & Chains\n","\n","In the previous example, the user input is sent directly to the Watsonx LLM, without using Langchain. This is a basic use case, but real applications are rarely so simple. When using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios. We will now replicate the previous example, but use an LLM chain. This allows us to:\n","\n","- Accept user input and contruct a prompt\n","- Generate multiple prompts from a collection of data points in a dataset "]},{"cell_type":"code","execution_count":9,"id":"310c2bbc","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["What is the capital of Sweden? = Stockholm\n","What is the capital of Mexico? = Mexico city\n","What is the capital of Vietnam? = Hanoi\n"]}],"source":["# Define the prompt template\n","prompt = PromptTemplate(\n","  input_variables=[\"country\"],\n","  template= \"What is the capital of {country}?\",\n",")\n","\n","try:\n","  # In order to use Langchain, we need to instantiate Langchain extension\n","  lc_llm_model = WatsonxLLM(model=llm_model)\n","  \n","  # Define a chain based on model and prompt\n","  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n","\n","  # Getting predictions\n","  countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n","  for country in countries:\n","    response = chain.run(country)\n","    print(prompt.format(country=country) + \" = \" + response.capitalize())\n","    sleep(0.5)\n","except Exception as e:\n","  print(e)\n"]},{"cell_type":"markdown","id":"918a9df3","metadata":{},"source":["## 3. Simple sequential chains\n","The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n","\n","LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."]},{"cell_type":"code","execution_count":10,"id":"ffda7c24","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Create two sequential prompts \n","pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n","pt2 = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=\"Answer the following question: {question}\",\n",")\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":11,"id":"23e4e1ee","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Instantiate 2 models (Note, these could be different models depending on use case)\n","# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n","model_1 = Model(\n","    model_id=\"google/flan-ul2\",\n","    params=params,\n","    credentials=creds,\n","    project_id=project_id\n",").to_langchain()\n","model_2 = Model(\n","    model_id=\"google/flan-ul2\",\n","    credentials=creds,\n","    project_id=project_id\n",").to_langchain()\n","\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":12,"id":"35de1e4d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Construct the sequential chain\n","prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n","prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n","qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":13,"id":"34586549","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3mWhat is the name of the smallest mammal?\u001b[0m\n","\u001b[33;1m\u001b[1;3mopossum\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# Run our chain with the topic: \"an animal\"\n","# Play around with providing different topics to see the output. eg. cars, the Roman empire\n","try:\n","  qa.run(\"an animal\")\n","except Exception as e:\n","  print(e)\n"]},{"cell_type":"markdown","id":"ed7c152d","metadata":{},"source":["## 4. Easy Loading of Documents Using Lang Chain\n","LangChain makes it easy to extract passages from documents so that you can answer questions based on your document's content. First download the example PDF file to your working folder: [what-is-generative-ai.pdf](https://github.com/ibm-build-lab/VAD-VAR-Workshop/blob/main/content/Watsonx/WatsonxAI/105/what-is-generative-ai.pdf)"]},{"cell_type":"code","execution_count":18,"id":"b1743ccd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Load PDF document\n","# pdf='what-is-generative-ai.pdf'\n","file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n","loaders = [PyPDFLoader(file_path)]\n","# loaders = [PyPDFLoader(pdf)]\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":23,"id":"9348f8c4","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95c10999a2e54739823ef900dfd92878","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb30336c75474e509980f8d54fb189b1","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8332efadb46d480eb1ff876c36288e73","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed12e7117d8b4d23ae9f1c529da9c940","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3ee027d428b445187c4348192ca02a9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e62c0ded0d1f459ba89ba907b2b9f355","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d0ab865f2e741e194437d29726780cc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1239bcc9b6704b21af1e56f8601d723f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9edc11e1dd8f43c99002fc24cd942ab2","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68c02622be4840b7a72a9e589c1535c8","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"487fc112138044b4a4397ecfdbfe56d7","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Index loaded PDF\n","index = VectorstoreIndexCreator(\n","    embedding=HuggingFaceEmbeddings(),\n","    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":24,"id":"43a890f9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Initialize watsonx google/flan-ul2 model\n","params = {\n","    GenParams.DECODING_METHOD: \"sample\",\n","    GenParams.TEMPERATURE: 0.2,\n","    GenParams.TOP_P: 1,\n","    GenParams.TOP_K: 100,\n","    GenParams.MIN_NEW_TOKENS: 50,\n","    GenParams.MAX_NEW_TOKENS: 300\n","}\n","model = Model(\n","    model_id=\"google/flan-ul2\",\n","    params=params,\n","    credentials=creds,\n","    project_id=project_id\n",").to_langchain()\n","\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":25,"id":"7d1a50d6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["# Init RAG chain\n","from langchain.chains import RetrievalQA\n","\n","chain = RetrievalQA.from_chain_type(llm=model, \n","                                    chain_type=\"stuff\", \n","                                    retriever=index.vectorstore.as_retriever(), \n","                                    input_key=\"question\")\n","print(\"Done.\")\n"]},{"cell_type":"code","execution_count":null,"id":"2b6efed8","metadata":{},"outputs":[],"source":["# Answer based on the document\n","res = chain.run(\"what is Machine Learning?\")\n","print(res)\n"]},{"cell_type":"markdown","id":"cc2679f2","metadata":{},"source":["Retrieval Augmented Generation (RAG) is a common AI use case. Many companies have vast amounts of data about which they want an AI system to answer questions, do searches or perform summarization tasks. We will learn more about RAG in lab 106."]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
