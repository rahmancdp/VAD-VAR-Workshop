{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Summarization with langchain\n",
        "\n",
        "Summarization of long documents is a common LLM use case. The issue that most often arises, however, is that there is a token limit for the model. (Max context window length). With langchain this can be worked around by chunking and recursive summarization."
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (4.41.2)\n",
            "Requirement already satisfied: chromadb in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.4.22)\n",
            "Requirement already satisfied: langchain in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.1.4)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (2.7.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.111.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.18.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (4.1.3)\n",
            "Requirement already satisfied: typer>=0.9.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.12.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.0.15)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
            "Requirement already satisfied: pyproject_hooks in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.10.3)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\n"
          ]
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.6.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
            "Requirement already satisfied: anyio<5,>=3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (4.4.0)\n",
            "Requirement already satisfied: coloredlogs in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (2.0)\n",
            "Requirement already satisfied: protobuf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.21.12)\n",
            "Requirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.13)\n",
            "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.56.4)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.46b0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.6.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.7.2)\n"
          ]
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.15.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "# First lets install dependencies (make sure already installed)\n",
        "!pip3 install transformers chromadb langchain\n"
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "# First import the dependencies we need:\n",
        "***REMOVED***\n",
        "***REMOVED***\n",
        "\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "***REMOVED***\n",
        "***REMOVED***\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "\n",
        "print(\"Done.\")\n"
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "# Get our API key, projectId and URL from .env\n",
        "\n",
        "project_id = \"\"\n",
        "api_key = \"\"\n",
        "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
        "\n",
        "\n",
        "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
        "    raise Exception(\"One or more environment variables are missing!\")\n",
        "else:\n",
        "    creds = {\n",
        "        \"url\": ibm_cloud_url,\n",
        "        \"apikey\": api_key \n",
        "***REMOVED***\n",
        "\n",
        "print(\"Done.\")\n"
      ]
***REMOVED***,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can take a [stuff](https://python.langchain.com/docs/modules/chains/document/stuff) or [map reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) approach to summarizing documents. We'll start with the simpler \"stuff\". Feel free to play around with changing the document URL and inference parameters to optimize the output. "
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading web document...\n",
            "Done.\n",
            "Initializing flan-ul2-20B model...\n",
            "Running summarization task...\n",
            "\n"
          ]
    ***REMOVED***,
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI—generative AI, in particular—has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI’s value is not limited to advances in the private sector. When implemented in a responsible way—where the technology is fully governed, privacy is protected, and decision-making is transparent and explainable—AI in government has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI tools likewise have the potential to supercharge digital modernization in by, for example, automating the migration of legacy software to more flexible cloud\n",
            "\n",
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "# Initialize llm and document loader:\n",
        "print(\"Loading web document...\")\n",
        "# Try out some other documents as well\n",
        "loader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\n",
        "doc = loader.load()\n",
        "print(\"Done.\")\n",
        "\n",
        "# You might need to tweak some of the runtime parameters to optimize the results.\n",
        "print(\"Initializing flan-ul2-20B model...\")\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: \"sample\",\n",
        "    GenParams.TEMPERATURE: 0.15,\n",
        "    GenParams.TOP_P: 1,\n",
        "    GenParams.TOP_K: 20,\n",
        "    GenParams.REPETITION_PENALTY: 1.0,\n",
        "    GenParams.MIN_NEW_TOKENS: 20,\n",
        "    GenParams.MAX_NEW_TOKENS: 205\n",
        "}\n",
        "\n",
        "flan_model = Model(\n",
        "    model_id=\"google/flan-ul2\",\n",
        "    params=params,\n",
        "    credentials=creds,\n",
        "    project_id=project_id\n",
        ").to_langchain()\n",
        "\n",
        "# Can use 'stuff' or 'map reduce'; \n",
        "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
        "\n",
        "print(\"Running summarization task...\\n\")\n",
        "\n",
        "res = chain.run(doc)\n",
        "\n",
        "print(res)\n",
        "print(\"\\nDone.\")\n"
      ]
***REMOVED***,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also combine several of the features we've seen previously, including prompt templates and chains. In the following block we load the document into a template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well. "
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing chain...\n",
            "Stuff chain with documents...\n",
            "Running summarization on stuffed document chain...\n",
            "\n",
            "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI—generative AI, in particular—has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI’s value is not limited to advances in the private sector. When implemented in a responsible way—where the technology is fully governed, privacy is protected, and decision-making is transparent and explainable—AI in government has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI tools likewise have the potential to supercharge digital modernization in by, for example, automating the migration of legacy software to more flexible cloud\n",
            "\n",
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "\n",
        "# Define prompt\n",
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define LLM chain\n",
        "print(\"Initializing chain...\")\n",
        "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "print(\"Stuff chain with documents...\")\n",
        "stuff_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
        ")\n",
        "\n",
        "print(\"Running summarization on stuffed document chain...\\n\")\n",
        "res = stuff_chain.run(doc)\n",
        "\n",
        "print(res)\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
***REMOVED***,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the output above should be the same as the previous block if using the same inference parameters and document URL. Now we will use the same stuff chain method to see how it behaves with multiple documents."
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 2nd article...\n",
            "Done.\n",
            "Running summarization on stuffed document chain.\n",
            "\n"
          ]
    ***REMOVED***,
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-12)\n",
            "Status code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-ul2': the number of input tokens 5309 cannot exceed the total tokens limit 4096 for this model\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"187fad7e645a95be0f3b544f045d4434\",\"status_code\":400}\n"
          ]
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-12)\n",
            "Status code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-ul2': the number of input tokens 5309 cannot exceed the total tokens limit 4096 for this model\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"187fad7e645a95be0f3b544f045d4434\",\"status_code\":400}\n",
            "\n",
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "# Add a new article\n",
        "print(\"Loading 2nd article...\")\n",
        "loader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\n",
        "doc_2 = loader_2.load() # Returns list\n",
        "print(\"Done.\")\n",
        "\n",
        "# Combine docs\n",
        "docs = doc + doc_2\n",
        "\n",
        "print(\"Running summarization on stuffed document chain.\\n\")\n",
        "try:\n",
        "  res = stuff_chain.run(docs)\n",
        "  print(res)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
***REMOVED***,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executing the above code should result in an error to the effect of `input tokens (7038) plus prefix length (0) must be < 4096` meaning that we have exceeded the model's token input length. This brings us to the next topic, \"map reduce\" which helps us solve this problem."
      ]
***REMOVED***,
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 3rd document...\n",
            "Init map chain...\n",
            "Init reduce chain...\n",
            "Stuff documents using reduce chain...\n",
            "Init chunk splitter...\n"
          ]
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41e82d68b4874b4ea1cf0d0d36d6ee7f",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8934dc10a6fa453f802b088b8e188813",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c73bb12b1767491dbd6844cdf0218577",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dd647078c59473d910506e2ec44ed93",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f5813c46ecd43ce803d28754d3c9586",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 3 chunks: \n",
            "Run map-reduce chain. This should take ~15-30 seconds...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n"
          ]
    ***REMOVED***,
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "578dbbae163a4cdc94591b62b716e261",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b0500a200f442898317e8ba23d67d66",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77edfa35f31440b4803909f14d1ca52e",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfe7ac59fbfc4dcaa49709883f166cbe",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a97a8752a04f11a2f85f64a0350979",
              "version_major": 2,
              "version_minor": 0
        ***REMOVED***,
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
      ***REMOVED***,
          "metadata": {},
          "output_type": "display_data"
    ***REMOVED***,
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Elapsed time: 11.96 seconds.\n",
            "\n",
            "Results from each chunk: \n",
            "\n",
            "1. No, I don't think so. I'm not interested in this topic. I'm interested in other topics.\n",
            "\n",
            "2. Federal employees are going to see AI tools show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services.\n",
            "\n",
            "3. How government agencies come to use generative AI and other innovative technologies in their operations will largely depend upon how the regulatory scheme unfolds\n",
            "\n",
            "\n",
            "\n",
            "Final output:\n",
            "\n",
            "No, I'm not interested in this topic. I'm interested in other topics. AI tools are going to show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services.\n",
            "\n",
            "Done.\n"
          ]
    ***REMOVED***
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "# Add a 3rd document\n",
        "print(\"Loading 3rd document...\")\n",
        "loader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\n",
        "doc_3 = loader_3.load()\n",
        "docs = docs + doc_3\n",
        "\n",
        "# Map\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{docs}\n",
        "Based on this list of docs, please identify the main themes \n",
        "Helpful Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "print(\"Init map chain...\")\n",
        "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
        "\n",
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{doc_summaries}\n",
        "Take these and distill it into a final, consolidated summary of the main themes. \n",
        "Helpful Answer:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "print(\"Init reduce chain...\")\n",
        "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "print(\"Stuff documents using reduce chain...\")\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        ")\n",
        "\n",
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000\n",
        ")\n",
        "\n",
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
        "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
        "print(\"Init chunk splitter...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\") # Hugging face tokenizer for flan-ul2\n",
        "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    print(f\"Using {len(split_docs)} chunks: \")\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
        "try:\n",
        "    t1_start = perf_counter()\n",
        "    results = map_reduce_chain(split_docs)\n",
        "    steps = results[\"intermediate_steps\"]\n",
        "    output = results[\"output_text\"]\n",
        "    t1_stop = perf_counter()\n",
        "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
        "\n",
        "    print(\"Results from each chunk: \\n\")\n",
        "    for idx, step in enumerate(steps):\n",
        "        print(f\"{idx + 1}. {step}\\n\")\n",
        "    \n",
        "    print(\"\\n\\nFinal output:\\n\")\n",
        "    print(output)\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
***REMOVED***,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
      ]
***REMOVED***
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10",
      "language": "python",
      "name": "python3"
***REMOVED***,
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
  ***REMOVED***,
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
***REMOVED***
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
